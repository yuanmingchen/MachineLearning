# 第二讲 词向量

这节课主要讲的是词向量，什么是词向量呢？说白了就是表示单词的向量，既然是表示单词的向量，那么向量必须要具有单词的含义。首先我们需要了解一下如何表示单词的含义。

### 1、怎样表示词语的含义？

##### （1）什么是词的含义？

含义的定义：

* 用词、短语等表示的思想。
* 一个人用词语、符号等想表达的想法。
* 在写作、艺术等作品中表达的思想：最普遍的语言

思维方式：   信号物（信号）&lt;=&gt;信号表示的含义（思想或事物）

##### （2）那么如何在计算中使用有用的含义呢？

常用解决方案:  
使用例如WordNet, 包含同义词集和超文本列表（上位词）  
的资源（“is a/an”关系）。

WordNet这种词义资源会提供词语的同义词、近义词集合，以及上位词（例如animal是panda的上位词，即panda is an animal）集合等，可以用于表示词义。

##### （3）使用像WordNet这样的资源的问题

* 作为词义资源很好，但是缺少词义之间的细微差别：例如
  “proficient”
  被列为
  “good”的同义词.，但是这仅仅在某些情况下是正确的
* 缺失词语的新含义：词语含义有更新时很难做到同步更新
* 词义的判断比较主观：对于一些比较模糊的同义词，是否将其列为同义词难以判断，无论选择哪种都太主观了。
* 需要人工创建和不断修改资源。
* 难以准确计算词语的相似度，只能知道两个词是否是同义词，但是相似度具体如何使未知的，例如dear和near同时被看做good的同义词，但是他们与good的相似度是不同的，但是在WordNet中无法区分。

##### （4）传统NLP表示单词的方法

在传统的NLP中,我们把单词看成离散的符号:hotel  
，conference  
，motel

单词可以用一个热向量\[热向量就是在所有维度中只有1个维度的值为‘1’，其余维度的值均为‘0’，只有一个热点，因此也叫一热点-向量\]表示（one-hot vectors）：

$$motel = [0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 1\ 0\ 0\ 0\ 0]$$

$$hotel = [0\ 0\ 0\ 0\ 0\ 0\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ 0\ 0]$$

向量维数 = 词汇中的单词数（比较大，应该在几万的级别）

##### （5）但是这样做会有问题（把单词表示成离散符号的问题）

让我们通过一个例子来看看到底有什么问题：

例如，在网络搜索中, 如果用户搜索  
“Seattle motel”,我们应该会匹  
配包含  
“Seattle hotel”的文档.

但是，如果把单词表示成离散符号，即用热向量来表示单词：

$$motel = [0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 1\ 0\ 0\ 0\ 0]$$

$$hotel = [0\ 0\ 0\ 0\ 0\ 0\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ 0\ 0]$$

这两个向量是正交的（任意两个热向量都是正交的，因为每个向量都只有一个1，且1的位置互不相同）

向量是正交的，我们就认为这两个向量无关，因此，对于热向量来说，并没有自然相似性的概念。

**解决办法：**

1. 可以通过WordNet资源中的同义词列表来获得相似性
2. 相反：学会在向量本身中编码相似性。

我们在之前也说了，采用WordNet这样的资源获得相似性会有不易更新、需要人工增改等诸多弊端，因此，我们不妨尝试第二种方法，在向量中就编码上词语的相似性！这就引出了我们这节课的主要内容，如何获得具有含义的词向量呢？

### 2、获得具有含义的词向量

这就让我们想到深度学习，只要建立相应的模型，把含有句子含义的语料作为训练资源，训练完成后就可以获得具有含义的词向量了！那么首先要解决的一个问题就是从语料中获得词的含义。

（1）用上下文表示单词

我们要想获得一个词的含义，并且这种含义需要计算机可以”理解“，那么如何表示词语的含义才能让计算机也可以“理解”呢？答案就是用上下文（词语周围的单词即为上下文）来表示单词含义！

**核心思想**：一个词的意义是由经常出现的词来表示的。这是现代统计NLP最成功的想法之一！

当单词  
w出现在一个文本中时, 它的上下文是这个词出现位  
置附近的词的集合 \(在固定的窗口大小内\)，我们可以使用w的多个上下文来建立w的表示：

!\[\]\(/assets/w'.png\)

##### （2）建立向量

我们将会为每个单词建立一个密集向量, 以使其类似于出  
现在相似上下文中的单词的词向量，即要求总是出现在相似上下文的单词的向量也具有较强的相似性（夹角较小，点积较大认为相似性大）。

注意：单词向量有时被称为单词嵌入  
（word  
embeddings）或单词表示（word representations）。

##### （3）Word2vec单词转向量概述

Word2vec单词转向量  
是学习单词向量的框  
架。

* 首先我们有大量的文本语料库。
* 在固定的词汇（所有单词的集合）中的每个词都用一个向量表示。
* 文本中的每个位置t都具有中心词c和上下文单词o。
* 使用C和O词向量的相似度计算O被赋给C的概率（反之亦
  然）
* 不断调整词向量以最大化这个概率

具体计算过程如下图所示：

![](/assets/contextWindow.png)对于每个位置$$𝑡=1,…,𝑇$$, 在固定大小m的窗口内预测上  
下文单词，给定中心词  
$$w_j$$:

那么整体的可能性（概率）为：

$$𝐿(\theta)=\prod_{t=1}^{T}\prod_{-m\leq j\leq m,j\neq0}𝑃(𝑤_{t+j}|𝑤_t;\theta)$$

其中$$\theta$$代表整个模型中的所有参数，具体有哪些参数稍后详细介绍。

$$𝐽(\theta)=-\frac{1}{T}\log𝐿(\theta)=-\frac{1}{T}\prod_{t=1}^{T}\prod_{-m\leq j\leq m,j\neq0}\log𝑃(𝑤_{t+j}|w_t;\theta)$$

最小化目标函数$$J(\theta)$$⟺最大化预测准确性$$L(\theta)$$

##### （4）优化过程（重点）

**目标**：我们想最小化目标函数:

$$𝐽(\theta)=-\frac{1}{T}\prod_{t=1}^{T}\prod_{-m\leq j\leq m,j\neq0}\log𝑃(𝑤_{t+j}|w_t;\theta)$$

**问题**：怎样计算$$𝑃(𝑤_{t+j}|w_t;\theta)$$？

**答案**:**对于每个词**$$w$$**我们使用两个向量**:

* 当$$w$$是中心词的时候，$$w$$的词向量是$$v_w$$
* 当$$w$$是中心词的时候，$$w$$的词向量是$$u_w$$

为什么用两个向量呢？我也不是特别清楚，教授似乎说是为了数学优化问题求解更加简便。

那么对于中心词  
c  
和一个上下文词  
o，我们有:

$$p(o|c)=\frac{\exp(u_o^Tv_c)}{\sum_{w\epsilon V}\exp(u_w^Tv_c)}$$

这个概率是给定中心词c，o是c的上下文单词的概率！注意到，这个概率与中心词c的位置是无关的，无论c的位置在哪里，只要o在c的上下文窗口之内，就认为o是c的上下文。

**参数问题**：现在我们可以详细看看$$\theta$$代表的参数了，我们假设窗口大小是固定的（实际上窗口大小是一个超参数，我们为了简化问题，假设窗口大小固定，从而忽略这个参数，只考虑向量的求解），不在我们考虑的参数范围之内，那么我们的参数就只有所有的词向量了，假设每个向量是d维的，词汇中一共有V个单词，并且不要忘了每个单词用两个向量表示，因此一共有2dV个参数变量，可以把$$\theta$$表示为以下形式：![](/assets/canshubiaoshi.png)

这里我们用**点积**来代表两个向量的相似度，但是点积的结果大小不一，无法直接用来当做概率，因此写作上述形式，就把所有的点积结果投射成了一个概率分布！

**softmax  
函数**：上述的形式被称为softmax  
函数形式，softmax  
函数是从n为向量到n为向量词的映射函数，它的函数表达式为：$$softmax(x_i)=\frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)}=p_i$$

说白了就是把所有的数据取指数之后除以所有数据取值数之和处理之后的输出类似于一个概率分布，所有数据之和为1，之所以要取值数，应该是为了扩大数据之间的差距，让数据大的数据更大，这样有利于筛选出更优秀的数据（这也是softmax函数中max的含义所在）。但是那些较小的$$x_i$$仍然分配了概率（尽管不是很多），这就是“soft”的含义了。

softmax  
函数的作用是将任意值  
$$x_i$$映射到一个概率分布  
$$p_i$$上，通过使用此函数，我们就可以把任意大小的数转换为概率分布。好了，softmax  
函数就介绍到这里来，很简单但经常会用到。

接下来我们来看如何优化（即最小化）目标函数$$J(\theta)$$，说到最小（大）化，我们很容易想起的一个概念就是**梯度**！

什么是梯度呢，一个函数在某一点的梯度，说白了就是函数值在该点的最快增大方向！梯度的反方向当然就是函数的最快下降方向了，我们称之为**负梯度**。

那么如何求解函数在某一点的梯度呢？其实很简单！只需要会一阶求导即可，我们来看一个例子：

假设有函数$$z=f(x,y)=x^3+3y^2$$，那么该函数的梯度为$$\{\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\}$$=$$\frac{\partial f}{\partial x}\overline i+\frac{\partial f}{\partial y}\overline j$$=$$3x^2\overline i+6y\overline j$$ 。

其中，$$\overline i$$代表$$x$$轴方向的单位向量（模长为1$$$$的向量称为单位向量），$$\overline j$$代表$$y$$轴方向的单位向量。

假设我们要求$$(1,2)$$这一点的梯度，只需要带入上式即可，即$$3\overline i+12\overline j$$方向即为梯度方向！即在$$(1,2)$$这一点，沿此方向前进函数$$z$$的大小增加最快，沿此方向的负方向前进函数$$z$$的大小减小最快！

上面的例子是以二元函数为例，多维函数也是一样的，只需要让函数对各个变量进行求偏导，各变量求导结果的方向合成（各变量求导结果乘以该变量维度的单位向量，对乘积进行求和即可）即为梯度方向。关于梯度就复习到这里。

接下来我们重新看我们的目标函数的优化问题，我们想要最小化目标函数$$J(\theta)$$，实际上就是在其负梯度方向进行移动，知道找到最小值为止，那么核心问题就是求解$$J(\theta)$$的梯度，要想求梯度，首先要搞清楚函数的参数有哪些，我们前面已经说过，该函数的所有参数$$\theta$$实际上是2V个d维向量，让$$J(\theta)$$对这2V个向量分别求偏导即可求出函数梯度！在这2V的向量中，有V个向量是中心向量，V个向量是上下文向量。函数对同一种向量的求偏导过程是相同的，因此，实际上我们只需要两个求偏导过程，即中心向量和上下文向量（实际上这两种向量的求偏导过程也很类似）。下面我们来看详细的求偏导推理过程！

